{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLStudy_exercise05_refactor.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lt8ByyprDWOL",
        "_7VQn4deDWON",
        "wrMSrxTjD7JH",
        "bbp0lWn1EHui",
        "yyhhTz9gEWyK",
        "xRx3EH-W5fmd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit ('mlstudy': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    },
    "interpreter": {
      "hash": "ec8436d89b325481430a6af2a07135c4515170aa95f16534205bdc289b4b1473"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G84V20dqZJUj"
      },
      "source": [
        "# 라이브러리 import\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from numpy.random import rand, multivariate_normal\n",
        "\n",
        "# 데이터 프레임 경고 없애기\n",
        "# pd.options.mode.chained_assignment = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt8ByyprDWOL"
      },
      "source": [
        "### **01. 로지스틱 함수 시각화**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSNn6p97ezNs"
      },
      "source": [
        "# 로지스틱 함수 시각화\n",
        "x = np.linspace(-6,6,101) \n",
        "y = 1.0/(1.0+np.exp(-x)) # 로지스틱 함수\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_title(\"logistic function\")\n",
        "ax.grid(True)\n",
        "ax.plot(x,y)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7VQn4deDWON"
      },
      "source": [
        "### **02. 트레이닝 셋 생성**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJMq_EwbT4oX"
      },
      "source": [
        "# 데이터 셋({x_n, y_n, type_n}) 만드는 함수(prepare_dataset) 선언하기\n",
        "# 개수가 N = N0+N1 이고 중심 좌표가 Mu이고 Variance를 가지고 있는 데이터 셋\n",
        "#\n",
        "# parameters = {\n",
        "#     \"N0\": t = 0인 데이터 개수},\n",
        "#     \"Mu0\": t = 0인 데이터 중심 (x,y) 좌표,\n",
        "#     \"N1\": t = 1인 데이터 개수,\n",
        "#     \"Mu1\": t = 1인 데이터 중심 (x,y) 좌표\n",
        "# }\n",
        "\n",
        "def prepare_dataset(parameters, variance):\n",
        "    N0 = parameters[\"N0\"]  # t = 0인 데이터 개수\n",
        "    Mu0 = parameters[\"Mu0\"] # t = 0인 데이터 중심 (x,y) 좌표\n",
        "    N1 = parameters[\"N1\"] # t = 1인 데이터 개수\n",
        "    Mu1 =  parameters[\"Mu1\"] # t = 1인 데이터 중심 (x,y) 좌표\n",
        "\n",
        "    # 공분산 행렬 C = [[Var(X),Cov(X,Y)], [Cov(Y,X),Var(Y)]\n",
        "    # Var(X): X축 분산, Var(Y): Y축 분산\n",
        "    # Cov(X,Y) = 0, Cov(Y,X) = 0인 공분산. 두 변수 X, Y는 서로 독립적. \n",
        "    # 공분산이 0이고 X축, Y축 분산이 같음\n",
        "    cov1 = np.array([[variance,0],[0,variance]])\n",
        "    cov2 = np.array([[variance,0],[0,variance]])\n",
        "\n",
        "    # t=+1인 데이터 생성\n",
        "    df1 = pd.DataFrame(multivariate_normal(Mu1,cov1,N1),columns=['x','y'])\n",
        "    df1['t'] = 1\n",
        "    # t=-1인 데이터 생성\n",
        "    df2 = pd.DataFrame(multivariate_normal(Mu0,cov2,N0),columns=['x','y'])\n",
        "    df2['t'] = 0\n",
        "    \n",
        "    # 생성된 데이터 합치기\n",
        "    df = pd.concat([df1,df2],ignore_index=True)\n",
        "\n",
        "    # df에 index를 랜덤으로 섞기\n",
        "    df = df.reindex(np.random.permutation(df.index))\n",
        "    \n",
        "    # df에 index를 처음부터 다시 라벨링하기 \n",
        "    # drop = True는 새로운 index를 추가로 만들지 않게 하기 위해서\n",
        "    df = df.reset_index(drop=True)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwjWiIdmDWOO"
      },
      "source": [
        "# 파라미터 \n",
        "# 개수가 N = N1+N2 이고 중심 좌표가 Mu이고 Variance를 가지고 있는 데이터 셋\n",
        "sample_parameters = {\n",
        "    \"N0\": 30, # t = 0인 데이터 개수\n",
        "    \"Mu0\": [-3,-3], # t = 0인 데이터 중심 (x,y) 좌표\n",
        "    \"N1\": 30, # t = 1인 데이터 개수\n",
        "    \"Mu1\": [7,7], # t = 1인 데이터 중심 [x,y] 좌표\n",
        "}\n",
        "\n",
        "df = prepare_dataset(sample_parameters, 50)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4,4))\n",
        "ax.set_xlim(-20,20)\n",
        "ax.set_ylim(-20,20)\n",
        "ax.scatter(df[df[\"t\"]==1].x, df[df[\"t\"]==1].y, label=\"t = +1\")\n",
        "ax.scatter(df[df[\"t\"]==0].x, df[df[\"t\"]==0].y, label=\"t = -1\")\n",
        "ax.legend()\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrMSrxTjD7JH"
      },
      "source": [
        "### **03. 퍼셉트론을 사용한 분류(4장)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_FwDFpnVfXh"
      },
      "source": [
        "def fit_perceptron(training_set, iteration_num, bias=None):\n",
        "    def calc_stochastic_gradient_descent_w(w):\n",
        "        # 확률적 기울기 하강\n",
        "        for _, data in training_set.iterrows():\n",
        "            x, y, t = data.x, data.y, data.t\n",
        "            t = t*2-1\n",
        "            phi = np.array([c, x, y]).reshape(-1, 1)\n",
        "            if np.dot(w.T, phi) * t <= 0:\n",
        "                w = w + t * phi  # w_new = w_old + t_n * phi_n\n",
        "        return w\n",
        "\n",
        "    def calc_error_rate():\n",
        "        # 잘못 분류된 데이터 비율(%) 계산하기\n",
        "        # err_num: 잘못된 데이터 개수\n",
        "        err_num = 0\n",
        "        for _, data in training_set.iterrows():\n",
        "            x, y, t = data.x, data.y, data.t\n",
        "            t = t*2-1\n",
        "            phi = np.array([c, x, y]).reshape(-1, 1)\n",
        "            if np.dot(w.T, phi) * t <= 0:\n",
        "                err_num += 1\n",
        "        err_rate = err_num / len(training_set) * 100\n",
        "\n",
        "        return err_rate\n",
        "    def calc_error():\n",
        "        # 잘못 분류된 데이터 비율(%) 계산하기\n",
        "        # err_num: 잘못된 데이터 개수\n",
        "        err = 0\n",
        "        for _, data in training_set.iterrows():\n",
        "            x, y, t = data.x, data.y, data.t\n",
        "            phi = np.array([c, x, y]).reshape(-1, 1)\n",
        "            t = t*2-1\n",
        "            if np.dot(w.T, phi) * t <= 0:\n",
        "                err_vector = np.dot(w.T, phi) * t * -1\n",
        "                err += err_vector.flatten()[0]\n",
        "        return err\n",
        "\n",
        "    # c는 bias항\n",
        "    c = 0\n",
        "    if bias is None:\n",
        "        c = 0.5 * (training_set.x.mean() + training_set.y.mean())\n",
        "    else:\n",
        "        c = bias\n",
        "\n",
        "    # w 초기값 설정하기\n",
        "    w = np.array([np.random.rand(),np.random.rand(),np.random.rand()]).reshape(-1, 1)\n",
        "\n",
        "    # parameter와 error rate를 기록하기 위한 DataFrame 선언\n",
        "    w_hist = pd.DataFrame([w.flatten()], columns=[\"w0\", \"w1\", \"w2\"])\n",
        "    err_rate_hist = pd.DataFrame([], columns=[\"err_rate\"])\n",
        "    err_hist = pd.DataFrame([], columns=[\"err\"])\n",
        "\n",
        "    for i in range(iteration_num):\n",
        "        w = calc_stochastic_gradient_descent_w(w)\n",
        "        w_hist = w_hist.append(\n",
        "            pd.Series(w.flatten(), index=[\"w0\", \"w1\", \"w2\"]), ignore_index=True\n",
        "        )\n",
        "\n",
        "        err_rate = calc_error_rate()\n",
        "        err_rate_hist = err_rate_hist.append(\n",
        "            pd.Series(err_rate, index=[\"err_rate\"]), ignore_index=True\n",
        "        )\n",
        "\n",
        "        err = calc_error()\n",
        "        err_hist = err_hist.append(\n",
        "            pd.Series(err, index=[\"err\"]), ignore_index=True\n",
        "        )\n",
        "    result = {\"w\": w, \"err_rate\": err_rate, \"bias\": c}\n",
        "\n",
        "    return result, w_hist, err_rate_hist, err_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbp0lWn1EHui"
      },
      "source": [
        "### **04. 로지스틱 회귀**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgFvvEfWDWOQ"
      },
      "source": [
        "$\n",
        "z = \\sigma (a) = \\sigma (\\mathbf{w}^T \\mathbf{\\Phi})\n",
        "\\\\\n",
        "\\mathbf{w}_{new} = \\mathbf{w}_{old} - (\\mathbf{\\Phi}^T \\mathbf{R} \\mathbf{\\Phi})^{-1} \\mathbf{\\Phi}^T (\\mathbf{z} - \\mathbf{t})\n",
        "\\\\\n",
        "\\mathbf{t}=\\begin{pmatrix} t_1 \\\\\\\\ \\vdots \\\\\\\\ t_N \\end{pmatrix}, \\mathbf{\\Phi} = \\begin{pmatrix} 1 & x_1 & y_1 \\\\\\\\ 1 & x_2 & y_2 \\\\\\\\ \\vdots & \\vdots & \\vdots \\\\\\\\ 1 & x_N & y_N \\end{pmatrix}, \\mathbf{z}=\\begin{pmatrix} z_1 \\\\\\\\ \\vdots \\\\\\\\ z_N \\end{pmatrix}, \\mathbf{R}= \\begin{pmatrix} z_1 (1 - z_1) &  & 0 \\\\\\\\  & \\ddots & \\\\\\\\ 0 &  & z_N (1 - z_N) \\end{pmatrix}  $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNBuK05hDWOR"
      },
      "source": [
        "def fit_logistics_regression(training_set, iteration_num, bias=None):    \n",
        "    def calc_phi():\n",
        "        phi =  training_set[['x','y']]\n",
        "        phi['bias'] = 1\n",
        "        phi = phi[[\"bias\",\"x\",\"y\"]] # phi의 열순서를 \"bias\", \"x\", \"y\"순으로 변경\n",
        "        phi = phi.values # datafame phi를 ndarray로 변환       \n",
        "        return phi\n",
        "    \n",
        "    def calc_t():\n",
        "        t = training_set[['t']] # training_set의 \"t\"열을 t에 대입\n",
        "        t = t.values # dataframe t를 ndarray로 변환\n",
        "        return t\n",
        "\n",
        "    def calc_IRLS(w, t, phi):\n",
        "        z = np.array([])\n",
        "        for line in phi:\n",
        "            a = np.dot(w.T, line)\n",
        "            z = np.append(z,[1.0/(1.0+np.exp(-a))])\n",
        "        R = np.diag(z*(1-z))\n",
        "        z = z.reshape(-1,1)\n",
        "        tmp1 = np.linalg.inv(np.linalg.multi_dot([phi.T, R,phi]))\n",
        "        tmp2 = np.dot(phi.T, (z-t))\n",
        "        w_new = w_old - np.dot(tmp1, tmp2)\n",
        "        \n",
        "        return w_new\n",
        "\n",
        "    def calc_err_rate(w):\n",
        "        err_num = 0\n",
        "        for _, point in training_set.iterrows():\n",
        "            x, y, t = point.x, point.y, point.t\n",
        "            t = t * 2 - 1\n",
        "            if t * (w[0] + w[1]*x + w[2]*y) < 0:\n",
        "                err_num += 1\n",
        "        err_rate = err_num * 100 / len(training_set)\n",
        "        return err_rate\n",
        "\n",
        "\n",
        "    phi = calc_phi()\n",
        "    t = calc_t()\n",
        "    w_old = np.array([[0.],[0.],[0.]])\n",
        "\n",
        "    for i in range(iteration_num):\n",
        "        w_new = calc_IRLS(w_old, t, phi)\n",
        "       \n",
        "        if np.dot((w_new-w_old).T, (w_new-w_old)) < 0.001 * np.dot(w_old.T, w_old):\n",
        "            # 파라미터의 변화가 0.1% 미만이 되면 종료\n",
        "            break\n",
        "       \n",
        "        w_old = w_new\n",
        "    err_rate = calc_err_rate(w_new)\n",
        "\n",
        "    #TODO Err 구하는 식\n",
        "    \n",
        "    result = {\n",
        "        \"w\": w_new,\n",
        "        \"err_rate\": err_rate,\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W2wP7aZ9fXO"
      },
      "source": [
        "# 결과를 시각화 하는 함수들\n",
        "\n",
        "#트레이닝 셋 시각화\n",
        "def plot_training_data(training_set,ax):\n",
        "    training_set1 = training_set[training_set['t']==1] # t=1인 트레이닝 셋\n",
        "    training_set2 = training_set[training_set['t']==0] # t=0인 트레이닝 셋\n",
        "    xmin, xmax = training_set.x.min()-5, training_set.x.max()+10\n",
        "    ymin, ymax = training_set.y.min()-5, training_set.y.max()+10\n",
        "    ax.set_xlim([xmin-1, xmax+1])\n",
        "    ax.set_ylim([ymin-1, ymax+1])\n",
        "    ax.scatter(training_set1.x, training_set1.y, marker='o')\n",
        "    ax.scatter(training_set2.x, training_set2.y, marker='x')\n",
        "\n",
        "# 퍼셉트론을 사용한 결과 시각화\n",
        "def plot_peceptron_graph(training_set, parameters, ax): \n",
        "    w0 = parameters[\"w\"][0]\n",
        "    w1 = parameters[\"w\"][1]\n",
        "    w2 = parameters[\"w\"][2]\n",
        "    bias = parameters[\"bias\"]\n",
        "    err_rate = parameters[\"err_rate\"]\n",
        "   \n",
        "    xmin, xmax = training_set.x.min()-5, training_set.x.max()+10\n",
        "    linex = np.arange(xmin-5, xmax+5)\n",
        "    liney = - linex * w1 / w2 - bias * w0 / w2\n",
        "    label = \"ERR %.2f%%\" % err_rate\n",
        "    ax.plot(linex, liney, label=label, color='red', linestyle='--')\n",
        "    ax.legend(loc=1)\n",
        "\n",
        "# 로지스틱 회귀분석을 사용한 결과 시각화\n",
        "def plot_logistic_regression_graph(training_set,parameters, ax):\n",
        "    w0 = parameters[\"w\"][0]\n",
        "    w1 = parameters[\"w\"][1]\n",
        "    w2 = parameters[\"w\"][2]\n",
        "    err_rate = parameters[\"err_rate\"]\n",
        "    \n",
        "    xmin, xmax = training_set.x.min()-5, training_set.x.max()+10\n",
        "    linex = np.arange(xmin-5, xmax+5)\n",
        "    liney = - linex * w1 / w2 - w0 / w2\n",
        "    label = \"ERR %.2f%%\" % err_rate\n",
        "    ax.plot(linex,liney ,label=label, color='blue')\n",
        "    ax.legend(loc=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyhhTz9gEWyK"
      },
      "source": [
        "### **05. 퍼셉트론, 로지스틱 회귀 시각화**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcglY71DAKLj"
      },
      "source": [
        "# 파라미터 \n",
        "# 개수가 N = N1+N2 이고 중심 좌표가 Mu이고 Variance를 가지고 있는 데이터 셋\n",
        "initial_parameters = {\n",
        "    \"N0\": 30, # t = 0인 데이터 개수\n",
        "    \"Mu0\": [-3,-3], # t = 0인 데이터 중심 (x,y) 좌표\n",
        "    \"N1\": 30, # t = 1인 데이터 개수\n",
        "    \"Mu1\": [7,7], # t = 1인 데이터 중심 [x,y] 좌표\n",
        "}\n",
        "variance_list = [5,10,30,50] # 양 클래스 공통의 분산(4종류의 분산으로 계산 실시)\n",
        "iteration_num = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMiofjA19g3o",
        "tags": []
      },
      "source": [
        "fig,axs = plt.subplots(2,2,figsize=(8,8))\n",
        "fig.suptitle('Blue: Logistic Regression, Red: Perceptron')\n",
        "for c, variance in enumerate(variance_list):\n",
        "    # 시각화 할 그래프 그리드 위치 설정\n",
        "    ax = axs[int(c/2),c%2]\n",
        "\n",
        "    # 트레이닝 셋 생성\n",
        "    training_set = prepare_dataset(initial_parameters, variance)\n",
        "\n",
        "    # 트레이닝 셋 시각화\n",
        "    plot_training_data(training_set,ax)\n",
        "    # 퍼셉트론을 사용하여 계산 및 시각화\n",
        "    perceptron_parameters, _, _, _ = fit_perceptron(training_set,iteration_num)\n",
        "    plot_peceptron_graph(training_set, perceptron_parameters, ax)\n",
        "\n",
        "    # 로지스틱 회귀 분석을 사용하여 계산 및 시각화\n",
        "    logistic_parameters = fit_logistics_regression(training_set, iteration_num)\n",
        "    plot_logistic_regression_graph(training_set,logistic_parameters, ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbiYJjHsH2Q3"
      },
      "source": [
        "# x,y위치 값이 t=1라벨로 분류될 확률 계산하기\n",
        "variance = 60\n",
        "x,y = (12,12)\n",
        "\n",
        "\n",
        "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
        "fig.suptitle(\"the probability of t=1 label where (x,y)\")\n",
        "\n",
        "# 시각화 할 그래프 그리드 위치 설정\n",
        "# 트레이닝 셋 생성\n",
        "training_set = prepare_dataset(initial_parameters, variance)\n",
        "\n",
        "# 트레이닝 셋 시각화\n",
        "plot_training_data(training_set,ax)\n",
        "\n",
        "# 로지스틱 회귀 분석을 사용하여 계산 및 시각화\n",
        "logistic_parameters = fit_logistics_regression(training_set, iteration_num)\n",
        "plot_logistic_regression_graph(training_set,logistic_parameters, ax)\n",
        "\n",
        "ax.scatter(x,y)\n",
        "\n",
        "w = [logistic_parameters[\"w\"][0],logistic_parameters[\"w\"][1],\n",
        "     logistic_parameters[\"w\"][2]]\n",
        "\n",
        "a = np.dot(np.array([1, x, y]), w)\n",
        "probability = 1.0/(1.0+np.exp(-a))\n",
        "ax.annotate('p = %.2f'%(probability), xy=(x, y),  xycoords='data',\n",
        "            xytext=(0.9, 0.5), textcoords='axes fraction',\n",
        "            arrowprops=dict(arrowstyle='->'),\n",
        "            horizontalalignment='right', verticalalignment='top',\n",
        "            )\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRx3EH-W5fmd"
      },
      "source": [
        "### **06. ROC 곡선**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VRaH_61aj4j"
      },
      "source": [
        "# ROC 계산 \n",
        "# ROC: Receiver Operating Caracteristic\n",
        "def calc_roc(training_set, parameter):\n",
        "    # 각 데이터 확률 계산\n",
        "    w = parameter['w']\n",
        "    training_set['probability'] = 0.0\n",
        "    \n",
        "    for index, line in training_set.iterrows():\n",
        "        a = np.dot(w.T, np.array([1, line.x, line.y]))\n",
        "        p = 1.0/(1.0+np.exp(-a))\n",
        "        training_set.loc[index, 'probability'] = p\n",
        "    training_set_with_p = training_set.sort_values(by=['probability'], ascending=[False]).reset_index(drop = True)\n",
        "\n",
        "    # 진양성률 및 위양성률 계산\n",
        "    # n(데이터 인덱스): [0,1, ... n ... ,N-1]\n",
        "    # t(데이터 속성): [t_1, t_2, ... t_n ... , t_N-1]\n",
        "    # 확률: [probability_1, probabilty_2, ... probability_n  ... , probability_N]\n",
        "    positives = len(training_set_with_p[training_set_with_p['t']==1]) # 양성 데이터 개수\n",
        "    negatives = len(training_set_with_p[training_set_with_p['t']==0]) # 음성 데이터 개수\n",
        "    tp = [0.0] * len(training_set_with_p) \n",
        "    fp = [0.0] * len(training_set_with_p) \n",
        "\n",
        "    # index가 n일 때  확률이 probabilty_n보다 큰지가 판단 기준\n",
        "    # 데이터가 정렬되어 있기 때문에, n보다 낮은 index에서는 항상 p값이 더 높음\n",
        "    for n, line in training_set_with_p.iterrows():\n",
        "        for c in np.arange(0,n):\n",
        "            if training_set_with_p.t[c] == 1:\n",
        "                tp[n] += 1\n",
        "            else:\n",
        "                fp[n] += 1\n",
        "        \n",
        "    tp_rate = np.array(tp) / positives # tp_rate(진양성률): true positive\n",
        "    fp_rate = np.array(fp) / negatives # fp_rate(위양성률): false positive\n",
        "    result = {\n",
        "        \"tp_rate\": tp_rate,\n",
        "        \"fp_rate\": fp_rate\n",
        "    }\n",
        "\n",
        "    return result, training_set_with_p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7s5dN_iGkJq"
      },
      "source": [
        "def plot_training_set(training_set, parameter, ax):\n",
        "    w0 = parameter[\"w\"][0]\n",
        "    w1 = parameter[\"w\"][1]\n",
        "    w2 = parameter[\"w\"][2]\n",
        "    err_rate = parameter[\"err_rate\"]\n",
        "\n",
        "    training_set1 = training_set[training_set['t']==1]\n",
        "    training_set2 = training_set[training_set['t']==0]\n",
        "    xmin, xmax = training_set.x.min()-5, training_set.x.max()+10\n",
        "    ymin, ymax = training_set.y.min()-5, training_set.y.max()+10\n",
        "    ax.set_ylim([ymin-1, ymax+1])\n",
        "    ax.set_xlim([xmin-1, xmax+1])\n",
        "\n",
        "    # 분류 데이터 표시\n",
        "    ax.scatter(training_set1.x, training_set1.y, marker='o')\n",
        "    ax.scatter(training_set2.x, training_set2.y, marker='x')\n",
        "\n",
        "    # P=0.5인 직선을 표시\n",
        "    linex = np.arange(xmin-5, xmax+5)\n",
        "    liney = - linex * w1 / w2 - w0 / w2\n",
        "    label = \"ERR %.2f%%\" % err_rate\n",
        "    ax.plot(linex,liney ,label=label, color='blue')\n",
        "    ax.legend(loc=1)\n",
        "\n",
        "# ROC곡선표시\n",
        "def plot_roc_curve(training_set, result, ax):\n",
        "    tp_rate = result[\"tp_rate\"]\n",
        "    fp_rate = result[\"fp_rate\"]\n",
        "    ax.set_ylim([0, 1])\n",
        "    ax.set_xlim([0, 1])\n",
        "    ax.set_xlabel(\"False positive rate\")\n",
        "    ax.set_ylabel(\"True positive rate\")\n",
        "    ax.plot(fp_rate, tp_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l7iTsF9LDFR"
      },
      "source": [
        "# 파라미터 \n",
        "# 개수가 N = N1+N2 이고 중심 좌표가 Mu이고 Variance를 가지고 있는 데이터 셋\n",
        "initial_parameters = {\n",
        "    \"N0\": 200, # t = 0인 데이터 개수\n",
        "    \"Mu0\": [-3,-3], # t = 0인 데이터 중심 (x,y) 좌표\n",
        "    \"N1\": 80, # t = 1인 데이터 개수\n",
        "    \"Mu1\": [9,9], # t = 1인 데이터 중심 (x,y) 좌표\n",
        "}\n",
        "\n",
        "variance_list = [80,200] # 양 클래스 공통의 분산(2종류의 분산으로 계산 실시)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMnBhOhWDWOY"
      },
      "source": [
        "training_set = prepare_dataset(initial_parameters, variance)\n",
        "parameter = fit_logistics_regression(training_set,100)\n",
        "result, training_set_with_p = calc_roc(training_set,parameter)\n",
        "training_set_with_p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKNPBZguGdUZ"
      },
      "source": [
        "# 로지스틱을 이용한 분류 시각화 및 ROC 곡선 시각화\n",
        "fig,axs = plt.subplots(2,2,figsize=(8,8))\n",
        "for c, variance in enumerate(variance_list):    \n",
        "    training_set = prepare_dataset(initial_parameters, variance)\n",
        "  \n",
        "    # 로지스틱 분류 계산 및 시각화\n",
        "    parameter = fit_logistics_regression(training_set,100)\n",
        "    plot_training_set(training_set,parameter, axs[0,c])\n",
        "    axs[0,c].set_title(\"variance = %d\"%(variance))\n",
        " \n",
        "    # ROC 곡선 계산 및 시각화\n",
        "    result, _ = calc_roc(training_set,parameter)\n",
        "    plot_roc_curve(training_set, result, axs[1,c])\n",
        "    axs[1,c].set_title(\"roc curve (variance = %d)\"%(variance)) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}